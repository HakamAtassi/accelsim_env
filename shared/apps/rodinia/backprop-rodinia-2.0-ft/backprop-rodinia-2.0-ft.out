Accel-Sim [build accelsim-commit-_modified_0.0_25-11-16-15-06-00]

        *** GPGPU-Sim Simulator Version 4.2.0  [build gpgpu-sim_git-commit-_modified_0.0] ***


GPGPU-Sim: Configuration options:

-save_embedded_ptx                      0 # saves ptx files embedded in binary as <n>.ptx
-keep                                   0 # keep intermediate files created by GPGPU-Sim when interfacing with external programs
-gpgpu_ptx_save_converted_ptxplus                    0 # Saved converted ptxplus to a file
-gpgpu_occupancy_sm_number                   86 # The SM number to pass to ptxas when getting register usage for computing GPU occupancy. This parameter is required in the config.
-ptx_opcode_latency_int           4,4,4,4,21 # Opcode latencies for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,19,25,145,32
-ptx_opcode_latency_fp           4,4,4,4,39 # Opcode latencies for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,30
-ptx_opcode_latency_dp      64,64,64,64,330 # Opcode latencies for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,335
-ptx_opcode_latency_sfu                   21 # Opcode latencies for SFU instructionsDefault 8
-ptx_opcode_latency_tesnor                   64 # Opcode latencies for Tensor instructionsDefault 64
-ptx_opcode_initiation_int            2,2,2,2,2 # Opcode initiation intervals for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,4,4,32,4
-ptx_opcode_initiation_fp            1,1,1,1,2 # Opcode initiation intervals for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,5
-ptx_opcode_initiation_dp      64,64,64,64,130 # Opcode initiation intervals for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,130
-ptx_opcode_initiation_sfu                    8 # Opcode initiation intervals for sfu instructionsDefault 8
-ptx_opcode_initiation_tensor                   64 # Opcode initiation intervals for tensor instructionsDefault 64
-cdp_latency         7200,8000,100,12000,1600 # CDP API latency <cudaStreamCreateWithFlags, cudaGetParameterBufferV2_init_perWarp, cudaGetParameterBufferV2_perKernel, cudaLaunchDeviceV2_init_perWarp, cudaLaunchDevicV2_perKernel>Default 7200,8000,100,12000,1600
-network_mode                           2 # Interconnection network mode
-inter_config_file                   mesh # Interconnection network config file
-icnt_in_buffer_limit                  512 # in_buffer_limit
-icnt_out_buffer_limit                  512 # out_buffer_limit
-icnt_subnets                           2 # subnets
-icnt_arbiter_algo                      1 # arbiter_algo
-icnt_verbose                           0 # inct_verbose
-icnt_grant_cycles                      1 # grant_cycles
-gpgpu_ptx_use_cuobjdump                    1 # Use cuobjdump to extract ptx and sass from binaries
-gpgpu_experimental_lib_support                    0 # Try to extract code from cuda libraries [Broken because of unknown cudaGetExportTable]
-checkpoint_option                      0 #  checkpointing flag (0 = no checkpoint)
-checkpoint_kernel                      1 #  checkpointing during execution of which kernel (1- 1st kernel)
-checkpoint_CTA                         0 #  checkpointing after # of CTA (< less than total CTA)
-resume_option                          0 #  resume flag (0 = no resume)
-resume_kernel                          0 #  Resume from which kernel (1= 1st kernel)
-resume_CTA                             0 #  resume from which CTA 
-checkpoint_CTA_t                       0 #  resume from which CTA 
-checkpoint_insn_Y                      0 #  resume from which CTA 
-gpgpu_ptx_convert_to_ptxplus                    0 # Convert SASS (native ISA) to ptxplus and run ptxplus
-gpgpu_ptx_force_max_capability                   86 # Force maximum compute capability
-gpgpu_ptx_inst_debug_to_file                    0 # Dump executed instructions' debug information to file
-gpgpu_ptx_inst_debug_file       inst_debug.txt # Executed instructions' debug output file
-gpgpu_ptx_inst_debug_thread_uid                    1 # Thread UID for executed instructions' debug output
-gpgpu_simd_model                       1 # 1 = post-dominator
-gpgpu_shader_core_pipeline              1536:32 # shader core pipeline config, i.e., {<nthread>:<warpsize>}
-gpgpu_tex_cache:l1  N:4:128:256,L:R:m:N:L,T:512:8,128:2 # per-shader L1 texture cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}
-gpgpu_const_cache:l1 N:128:64:8,L:R:f:N:L,S:2:64,4 # per-shader L1 constant memory cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:il1     N:64:128:16,L:R:f:N:L,S:2:48,4 # shader L1 instruction cache config  {<sector?>:<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:dl1     S:4:128:256,L:T:m:L:L,A:384:48,16:0,32 # per-shader L1 data cache config  {<sector?>:<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:<fifo_entry>,<data_port_width> | none}
-gpgpu_l1_cache_write_ratio                   25 # L1D write ratio
-gpgpu_l1_banks                         4 # The number of L1 cache banks
-gpgpu_l1_banks_byte_interleaving                   32 # l1 banks byte interleaving granularity
-gpgpu_l1_banks_hashing_function                    0 # l1 banks hashing function
-gpgpu_l1_latency                      39 # L1 Hit Latency
-gpgpu_smem_latency                    29 # smem Latency
-gpgpu_cache:dl1PrefL1                 none # per-shader L1 data cache config  {<sector?>:<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:<fifo_entry>,<data_port_width> | none}
-gpgpu_cache:dl1PrefShared                 none # per-shader L1 data cache config  {<sector?>:<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:<fifo_entry>,<data_port_width> | none}
-gpgpu_gmem_skip_L1D                    0 # global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)
-gpgpu_perfect_mem                      0 # enable perfect memory mode (no cache miss)
-n_regfile_gating_group                    4 # group of lanes that should be read/written together)
-gpgpu_clock_gated_reg_file                    0 # enable clock gated reg file for power calculations
-gpgpu_clock_gated_lanes                    0 # enable clock gated lanes for power calculations
-gpgpu_shader_registers                65536 # Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)
-gpgpu_registers_per_block                65536 # Maximum number of registers per CTA. (default 8192)
-gpgpu_ignore_resources_limitation                    0 # gpgpu_ignore_resources_limitation (default 0)
-gpgpu_shader_cta                      32 # Maximum number of concurrent CTAs in shader (default 32)
-gpgpu_num_cta_barriers                   16 # Maximum number of named barriers per CTA (default 16)
-gpgpu_n_clusters                      46 # number of processing clusters
-gpgpu_n_cores_per_cluster                    1 # number of simd cores per cluster
-gpgpu_n_cluster_ejection_buffer_size                   32 # number of packets in ejection buffer
-gpgpu_n_ldst_response_buffer_size                    2 # number of response packets in ld/st unit ejection buffer
-gpgpu_shmem_per_block                49152 # Size of shared memory per thread block or CTA (default 48kB)
-gpgpu_shmem_size                  102400 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_option      0,8,16,32,64,100 # Option list of shared memory sizes
-gpgpu_unified_l1d_size                  128 # Size of unified data cache(L1D + shared memory) in KB
-gpgpu_adaptive_cache_config                    1 # adaptive_cache_config
-gpgpu_shmem_sizeDefault               102400 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefL1                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefShared                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_num_banks                   32 # Number of banks in the shared memory in each shader core (default 16)
-gpgpu_shmem_limited_broadcast                    0 # Limit shared memory to do one broadcast per cycle (default on)
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_mem_unit_ports                    1 # The number of memory transactions allowed per core cycle
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_warpdistro_shader                   -1 # Specify which shader core to collect the warp size distribution from
-gpgpu_warp_issue_shader                    0 # Specify which shader core to collect the warp issue distribution from
-gpgpu_local_mem_map                    1 # Mapping from local memory space address to simulated GPU physical address space (default = enabled)
-gpgpu_num_reg_banks                    8 # Number of register banks (default = 8)
-gpgpu_reg_bank_use_warp_id                    0 # Use warp ID in mapping registers to banks (default = off)
-gpgpu_sub_core_model                    1 # Sub Core Volta/Pascal model (default = off)
-gpgpu_enable_specialized_operand_collector                    0 # enable_specialized_operand_collector
-gpgpu_operand_collector_num_units_sp                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_dp                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_sfu                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_int                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_tensor_core                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_mem                    2 # number of collector units (default = 2)
-gpgpu_operand_collector_num_units_gen                    8 # number of collector units (default = 0)
-gpgpu_operand_collector_num_in_ports_sp                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_gen                    8 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sp                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_gen                    8 # number of collector unit in ports (default = 0)
-gpgpu_coalesce_arch                   86 # Coalescing arch (GT200 = 13, Fermi = 20)
-gpgpu_num_sched_per_core                    4 # Number of warp schedulers per core
-gpgpu_max_insn_issue_per_warp                    1 # Max number of instructions that can be issued per warp in one cycle by scheduler (either 1 or 2)
-gpgpu_dual_issue_diff_exec_units                    1 # should dual issue use two different execution unit resources (Default = 1)
-gpgpu_simt_core_sim_order                    1 # Select the simulation order of cores in a cluster (0=Fix, 1=Round-Robin)
-gpgpu_pipeline_widths 4,4,4,4,4,4,4,4,4,4,8,4,4 # Pipeline widths ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_INT,OC_EX_SFU,OC_EX_MEM,EX_WB,ID_OC_TENSOR_CORE,OC_EX_TENSOR_CORE
-gpgpu_tensor_core_avail                    1 # Tensor Core Available (default=0)
-gpgpu_num_sp_units                     4 # Number of SP units (default=1)
-gpgpu_num_dp_units                     4 # Number of DP units (default=0)
-gpgpu_num_int_units                    4 # Number of INT units (default=0)
-gpgpu_num_sfu_units                    4 # Number of SF units (default=1)
-gpgpu_num_tensor_core_units                    4 # Number of tensor_core units (default=1)
-gpgpu_num_mem_units                    1 # Number if ldst units (default=1) WARNING: not hooked up to anything
-gpgpu_scheduler                      lrr # Scheduler configuration: < lrr | gto | two_level_active > If two_level_active:<num_active_warps>:<inner_prioritization>:<outer_prioritization>For complete list of prioritization values see shader.h enum scheduler_prioritization_typeDefault: gto
-gpgpu_concurrent_kernel_sm                    0 # Support concurrent kernels on a SM (default = disabled)
-gpgpu_perfect_inst_const_cache                    1 # perfect inst and const cache mode, so all inst and const hits in the cache(default = disabled)
-gpgpu_inst_fetch_throughput                    4 # the number of fetched intruction per warp each cycle
-gpgpu_reg_file_port_throughput                    2 # the number ports of the register file
-specialized_unit_1         1,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_2       1,4,200,4,4,TEX # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_3     1,4,32,4,4,TENSOR # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_4         1,4,4,4,4,UDP # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_5         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_6         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_7         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_8         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-gpgpu_perf_sim_memcpy                    1 # Fill the L2 cache on memcpy
-gpgpu_simple_dram_model                    0 # simple_dram_model with fixed latency and BW
-gpgpu_dram_scheduler                    1 # 0 = fifo, 1 = FR-FCFS (defaul)
-gpgpu_dram_partition_queues          64:64:64:64 # i2$:$2d:d2$:$2i
-l2_ideal                               0 # Use a ideal L2 cache that always hit
-gpgpu_cache:dl2     S:64:128:16,L:B:m:L:P,A:192:4,32:0,32 # unified banked L2 data cache config  {<sector?>:<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:<fifo_entry>,<data_port_width>
-gpgpu_cache:dl2_texture_only                    0 # L2 cache used for texture only
-gpgpu_n_mem                           16 # number of memory modules (e.g. memory controllers) in gpu
-gpgpu_n_sub_partition_per_mchannel                    2 # number of memory subpartition in each memory module
-gpgpu_n_mem_per_ctrlr                    1 # number of memory chips per memory controller
-gpgpu_memlatency_stat                   14 # track and display latency statistics 0x2 enables MC, 0x4 enables queue logs
-gpgpu_frfcfs_dram_sched_queue_size                   64 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_return_queue_size                  192 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_buswidth                    2 # default = 4 bytes (8 bytes per cycle at DDR)
-gpgpu_dram_burst_length                   16 # Burst length of each DRAM request (default = 4 data bus cycle)
-dram_data_command_freq_ratio                    4 # Frequency ratio between DRAM data bus and command bus (default = 2 times, i.e. DDR)
-gpgpu_dram_timing_opt nbk=16:CCD=4:RRD=12:RCD=24:RAS=55:RP=24:RC=78:CL=24:WL=8:CDLR=10:WR=24:nbkgrp=4:CCDL=6:RTPL=4 # DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}
-gpgpu_l2_rop_latency                  187 # ROP queue latency (default 85)
-dram_latency                         254 # DRAM latency (default 30)
-dram_dual_bus_interface                    0 # dual_bus_interface (default = 0) 
-dram_bnk_indexing_policy                    0 # dram_bnk_indexing_policy (0 = normal indexing, 1 = Xoring with the higher bits) (Default = 0)
-dram_bnkgrp_indexing_policy                    1 # dram_bnkgrp_indexing_policy (0 = take higher bits, 1 = take lower bits) (Default = 0)
-dram_seperate_write_queue_enable                    0 # Seperate_Write_Queue_Enable
-dram_write_queue_size             32:28:16 # Write_Queue_Size
-dram_elimnate_rw_turnaround                    0 # elimnate_rw_turnaround i.e set tWTR and tRTW = 0
-icnt_flit_size                        40 # icnt_flit_size
-SST_mode                               0 # SST mode
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RBBBCCCC.BCCSSSSS # mapping memory address to dram model {dramid@<start bit>;<memory address map>}
-gpgpu_mem_addr_test                    0 # run sweep test to check address mapping for aliased address
-gpgpu_mem_address_mask                    1 # 0 = old addressing mask, 1 = new addressing mask, 2 = new add. mask + flipped bank sel and chip sel bits
-gpgpu_memory_partition_indexing                    2 # 0 = no indexing, 1 = bitwise xoring, 2 = IPoly, 3 = custom indexing
-accelwattch_xml_file accelwattch_sass_sim.xml # AccelWattch XML file
-power_simulation_enabled                    0 # Turn on power simulator (1=On, 0=Off)
-power_per_cycle_dump                    0 # Dump detailed power output each cycle
-hw_perf_file_name            hw_perf.csv # Hardware Performance Statistics file
-hw_perf_bench_name                       # Kernel Name in Hardware Performance Statistics file
-power_simulation_mode                    0 # Switch performance counter input for power simulation (0=Sim, 1=HW, 2=HW-Sim Hybrid)
-dvfs_enabled                           0 # Turn on DVFS for power model
-aggregate_power_stats                    0 # Accumulate power across all kernels
-accelwattch_hybrid_perfsim_L1_RH                    0 # Get L1 Read Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_RM                    0 # Get L1 Read Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_WH                    0 # Get L1 Write Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_WM                    0 # Get L1 Write Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_RH                    0 # Get L2 Read Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_RM                    0 # Get L2 Read Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_WH                    0 # Get L2 Write Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_WM                    0 # Get L2 Write Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_CC_ACC                    0 # Get Constant Cache Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_SHARED_ACC                    0 # Get Shared Memory Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_DRAM_RD                    0 # Get DRAM Reads for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_DRAM_WR                    0 # Get DRAM Writes for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_NOC                    0 # Get Interconnect Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_PIPE_DUTY                    0 # Get Pipeline Duty Cycle Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_NUM_SM_IDLE                    0 # Get Number of Idle SMs for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_CYCLES                    0 # Get Executed Cycles for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_VOLTAGE                    0 # Get Chip Voltage for Accelwattch-Hybrid from Accel-Sim
-power_trace_enabled                    0 # produce a file for the power trace (1=On, 0=Off)
-power_trace_zlevel                     6 # Compression level of the power trace output log (0=no comp, 9=highest)
-steady_power_levels_enabled                    0 # produce a file for the steady power levels (1=On, 0=Off)
-steady_state_definition                  8:4 # allowed deviation:number of samples
-gpgpu_max_cycle                        0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_insn                         0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_cta                          0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_completed_cta                    0 # terminates gpu simulation early (0 = no limit)
-gpgpu_runtime_stat                   500 # display runtime statistics such as dram utilization {<freq>:<flag>}
-liveness_message_freq                    1 # Minimum number of seconds between simulation liveness messages (0 = always print)
-gpgpu_compute_capability_major                    8 # Major compute capability version number
-gpgpu_compute_capability_minor                    6 # Minor compute capability version number
-gpgpu_flush_l1_cache                    1 # Flush L1 cache at the end of each kernel call
-gpgpu_flush_l2_cache                    0 # Flush L2 cache at the end of each kernel call
-gpgpu_deadlock_detect                    1 # Stop the simulation at deadlock (1=on (default), 0=off)
-gpgpu_ptx_instruction_classification                    0 # if enabled will classify ptx instruction types per kernel (Max 255 kernels now)
-gpgpu_ptx_sim_mode                     0 # Select between Performance (default) or Functional simulation (1)
-gpgpu_clock_domains 1132:1132:1132:3500.5 # Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}
-gpgpu_max_concurrent_kernel                  128 # maximum kernels that can run concurrently on GPU, set this value according to max resident grids for your compute capability
-gpgpu_cflog_interval                    0 # Interval between each snapshot in control flow logger
-visualizer_enabled                     0 # Turn on visualizer output (1=On, 0=Off)
-visualizer_outputfile                 NULL # Specifies the output log file for visualizer
-visualizer_zlevel                      6 # Compression level of the visualizer output log (0=no comp, 9=highest)
-gpgpu_stack_size_limit                 1024 # GPU thread stack size
-gpgpu_heap_size_limit              8388608 # GPU malloc heap size 
-gpgpu_runtime_sync_depth_limit                    2 # GPU device runtime synchronize depth
-gpgpu_runtime_pending_launch_count_limit                 2048 # GPU device runtime pending launch count
-trace_enabled                          0 # Turn on traces
-trace_components                    none # comma seperated list of traces to enable. Complete list found in trace_streams.tup. Default none
-trace_sampling_core                    0 # The core which is printed using CORE_DPRINTF. Default 0
-trace_sampling_memory_partition                   -1 # The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)
-enable_ptx_file_line_stats                    1 # Turn on PTX source line statistic profiling. (1 = On)
-ptx_line_stats_filename gpgpu_inst_stats.txt # Output file for PTX source line statistics.
-gpgpu_kernel_launch_latency                 5000 # Kernel launch latency in cycles. Default: 0
-gpgpu_cdp_enabled                      0 # Turn on CDP
-gpgpu_TB_launch_latency                    0 # thread block launch latency in cycles. Default: 0
-trace               /root/Repos/shared/accel-sim-framework/hw_run/traces/device-0/12.8/backprop-rodinia-2.0-ft/4096___data_result_4096_txt/traces/kernelslist.g # traces kernel filetraces kernel file directory
-trace_opcode_latency_initiation_int                  2,2 # Opcode latencies and initiation for integers in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_sp                  2,1 # Opcode latencies and initiation for sp in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_dp                64,64 # Opcode latencies and initiation for dp in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_sfu                 21,8 # Opcode latencies and initiation for sfu in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_tensor                32,32 # Opcode latencies and initiation for tensor in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_spec_op_1                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_2                200,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_3                32,32 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_4                  4,1 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_5                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_6                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_7                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_8                  4,4 # specialized unit config <latency,initiation>
DRAM Timing Options:
nbk                                    16 # number of banks
CCD                                     4 # column to column delay
RRD                                    12 # minimal delay between activation of rows in different banks
RCD                                    24 # row to column delay
RAS                                    55 # time needed to activate row
RP                                     24 # time needed to precharge (deactivate) row
RC                                     78 # row cycle time
CDLR                                   10 # switching from write to read (changes tWTR)
WR                                     24 # last data-in to row precharge
CL                                     24 # CAS latency
WL                                      8 # Write latency
nbkgrp                                  4 # number of bank groups
CCDL                                    6 # column to column delay between accesses to different bank groups
RTPL                                    4 # read to precharge delay between accesses to different bank groups
Total number of memory sub partition = 32
addr_dec_mask[CHIP]  = 0000000000000f00 	high:12 low:8
addr_dec_mask[BK]    = 0000000000070080 	high:19 low:7
addr_dec_mask[ROW]   = 00000000fff80000 	high:32 low:19
addr_dec_mask[COL]   = 000000000000f07f 	high:16 low:0
addr_dec_mask[BURST] = 000000000000001f 	high:5 low:0
sub_partition_id_mask = 0000000000000080
GPGPU-Sim uArch: clock freqs: 1132000000.000000:1132000000.000000:1132000000.000000:3500500000.000000
GPGPU-Sim uArch: clock periods: 0.00000000088339222615:0.00000000088339222615:0.00000000088339222615:0.00000000028567347522
*** Initializing Memory Statistics ***
GPGPU-Sim uArch: performance model initialization complete.
launching memcpy command : MemcpyHtoD,0x00007f15b8e00000,16388
launching memcpy command : MemcpyHtoD,0x00007f15b8e04400,278596
Processing kernel /root/Repos/shared/accel-sim-framework/hw_run/traces/device-0/12.8/backprop-rodinia-2.0-ft/4096___data_result_4096_txt/traces/kernel-1-ctx_0x55b7bc2df3e0.traceg.xz
-kernel name = _Z22bpnn_layerforward_CUDAPfS_S_S_ii
-kernel id = 1
-grid dim = (1,256,1)
-block dim = (16,16,1)
-shmem = 1088
-nregs = 20
-binary version = 86
-cuda stream id = 0
-shmem base_addr = 0x00007f15d5000000
-local mem base_addr = 0x00007f15d3000000
-nvbit version = 1.7.6
-accelsim tracer version = 5
-enable lineinfo = 0
Header info loaded for kernel command : /root/Repos/shared/accel-sim-framework/hw_run/traces/device-0/12.8/backprop-rodinia-2.0-ft/4096___data_result_4096_txt/traces/kernel-1-ctx_0x55b7bc2df3e0.traceg.xz
launching kernel name: _Z22bpnn_layerforward_CUDAPfS_S_S_ii uid: 1 cuda_stream_id: 0
GPGPU-Sim uArch: Shader 0 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
GPGPU-Sim uArch: CTA/core = 6, limited by: threads
GPGPU-Sim: Reconfigure L1 cache to 120KB
thread block = 0,0,0
GPGPU-Sim uArch: Shader 1 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,1,0
GPGPU-Sim uArch: Shader 2 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,2,0
GPGPU-Sim uArch: Shader 3 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,3,0
GPGPU-Sim uArch: Shader 4 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,4,0
GPGPU-Sim uArch: Shader 5 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,5,0
GPGPU-Sim uArch: Shader 6 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,6,0
GPGPU-Sim uArch: Shader 7 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,7,0
GPGPU-Sim uArch: Shader 8 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,8,0
GPGPU-Sim uArch: Shader 9 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,9,0
GPGPU-Sim uArch: Shader 10 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,10,0
GPGPU-Sim uArch: Shader 11 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,11,0
GPGPU-Sim uArch: Shader 12 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,12,0
GPGPU-Sim uArch: Shader 13 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,13,0
GPGPU-Sim uArch: Shader 14 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,14,0
GPGPU-Sim uArch: Shader 15 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,15,0
GPGPU-Sim uArch: Shader 16 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,16,0
GPGPU-Sim uArch: Shader 17 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,17,0
GPGPU-Sim uArch: Shader 18 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,18,0
GPGPU-Sim uArch: Shader 19 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,19,0
GPGPU-Sim uArch: Shader 20 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,20,0
GPGPU-Sim uArch: Shader 21 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,21,0
GPGPU-Sim uArch: Shader 22 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,22,0
GPGPU-Sim uArch: Shader 23 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,23,0
GPGPU-Sim uArch: Shader 24 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,24,0
GPGPU-Sim uArch: Shader 25 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,25,0
GPGPU-Sim uArch: Shader 26 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,26,0
GPGPU-Sim uArch: Shader 27 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,27,0
GPGPU-Sim uArch: Shader 28 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,28,0
GPGPU-Sim uArch: Shader 29 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,29,0
GPGPU-Sim uArch: Shader 30 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,30,0
GPGPU-Sim uArch: Shader 31 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,31,0
GPGPU-Sim uArch: Shader 32 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,32,0
GPGPU-Sim uArch: Shader 33 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,33,0
GPGPU-Sim uArch: Shader 34 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,34,0
GPGPU-Sim uArch: Shader 35 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,35,0
GPGPU-Sim uArch: Shader 36 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,36,0
GPGPU-Sim uArch: Shader 37 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,37,0
GPGPU-Sim uArch: Shader 38 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,38,0
GPGPU-Sim uArch: Shader 39 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,39,0
GPGPU-Sim uArch: Shader 40 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,40,0
GPGPU-Sim uArch: Shader 41 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,41,0
GPGPU-Sim uArch: Shader 42 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,42,0
GPGPU-Sim uArch: Shader 43 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,43,0
GPGPU-Sim uArch: Shader 44 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,44,0
GPGPU-Sim uArch: Shader 45 bind to kernel 1 '_Z22bpnn_layerforward_CUDAPfS_S_S_ii'
thread block = 0,45,0
thread block = 0,46,0
thread block = 0,47,0
thread block = 0,48,0
thread block = 0,49,0
thread block = 0,50,0
thread block = 0,51,0
thread block = 0,52,0
thread block = 0,53,0
thread block = 0,54,0
thread block = 0,55,0
thread block = 0,56,0
thread block = 0,57,0
thread block = 0,58,0
thread block = 0,59,0
thread block = 0,60,0
thread block = 0,61,0
thread block = 0,62,0
thread block = 0,63,0
thread block = 0,64,0
thread block = 0,65,0
thread block = 0,66,0
thread block = 0,67,0
thread block = 0,68,0
thread block = 0,69,0
thread block = 0,70,0
thread block = 0,71,0
thread block = 0,72,0
thread block = 0,73,0
thread block = 0,74,0
thread block = 0,75,0
thread block = 0,76,0
thread block = 0,77,0
thread block = 0,78,0
thread block = 0,79,0
thread block = 0,80,0
thread block = 0,81,0
thread block = 0,82,0
thread block = 0,83,0
thread block = 0,84,0
thread block = 0,85,0
thread block = 0,86,0
thread block = 0,87,0
thread block = 0,88,0
thread block = 0,89,0
thread block = 0,90,0
thread block = 0,91,0
thread block = 0,92,0
thread block = 0,93,0
thread block = 0,94,0
thread block = 0,95,0
thread block = 0,96,0
thread block = 0,97,0
thread block = 0,98,0
thread block = 0,99,0
thread block = 0,100,0
thread block = 0,101,0
thread block = 0,102,0
thread block = 0,103,0
thread block = 0,104,0
thread block = 0,105,0
thread block = 0,106,0
thread block = 0,107,0
thread block = 0,108,0
thread block = 0,109,0
thread block = 0,110,0
thread block = 0,111,0
thread block = 0,112,0
thread block = 0,113,0
thread block = 0,114,0
thread block = 0,115,0
thread block = 0,116,0
thread block = 0,117,0
thread block = 0,118,0
thread block = 0,119,0
thread block = 0,120,0
thread block = 0,121,0
thread block = 0,122,0
thread block = 0,123,0
thread block = 0,124,0
thread block = 0,125,0
thread block = 0,126,0
thread block = 0,127,0
thread block = 0,128,0
thread block = 0,129,0
thread block = 0,130,0
thread block = 0,131,0
thread block = 0,132,0
thread block = 0,133,0
thread block = 0,134,0
thread block = 0,135,0
thread block = 0,136,0
thread block = 0,137,0
thread block = 0,138,0
thread block = 0,139,0
thread block = 0,140,0
thread block = 0,141,0
thread block = 0,142,0
thread block = 0,143,0
thread block = 0,144,0
thread block = 0,145,0
thread block = 0,146,0
thread block = 0,147,0
thread block = 0,148,0
thread block = 0,149,0
thread block = 0,150,0
thread block = 0,151,0
thread block = 0,152,0
thread block = 0,153,0
thread block = 0,154,0
thread block = 0,155,0
thread block = 0,156,0
thread block = 0,157,0
thread block = 0,158,0
thread block = 0,159,0
thread block = 0,160,0
thread block = 0,161,0
thread block = 0,162,0
thread block = 0,163,0
thread block = 0,164,0
thread block = 0,165,0
thread block = 0,166,0
thread block = 0,167,0
thread block = 0,168,0
thread block = 0,169,0
thread block = 0,170,0
thread block = 0,171,0
thread block = 0,172,0
thread block = 0,173,0
thread block = 0,174,0
thread block = 0,175,0
thread block = 0,176,0
thread block = 0,177,0
thread block = 0,178,0
thread block = 0,179,0
thread block = 0,180,0
thread block = 0,181,0
thread block = 0,182,0
thread block = 0,183,0
thread block = 0,184,0
thread block = 0,185,0
thread block = 0,186,0
thread block = 0,187,0
thread block = 0,188,0
thread block = 0,189,0
thread block = 0,190,0
thread block = 0,191,0
thread block = 0,192,0
thread block = 0,193,0
thread block = 0,194,0
thread block = 0,195,0
thread block = 0,196,0
thread block = 0,197,0
thread block = 0,198,0
thread block = 0,199,0
thread block = 0,200,0
thread block = 0,201,0
thread block = 0,202,0
thread block = 0,203,0
thread block = 0,204,0
thread block = 0,205,0
thread block = 0,206,0
thread block = 0,207,0
thread block = 0,208,0
thread block = 0,209,0
thread block = 0,210,0
thread block = 0,211,0
thread block = 0,212,0
thread block = 0,213,0
thread block = 0,214,0
thread block = 0,215,0
thread block = 0,216,0
thread block = 0,217,0
thread block = 0,218,0
thread block = 0,219,0
thread block = 0,220,0
thread block = 0,221,0
thread block = 0,222,0
thread block = 0,223,0
thread block = 0,224,0
thread block = 0,225,0
thread block = 0,226,0
thread block = 0,227,0
thread block = 0,228,0
thread block = 0,229,0
thread block = 0,230,0
thread block = 0,231,0
thread block = 0,232,0
thread block = 0,233,0
thread block = 0,234,0
thread block = 0,235,0
thread block = 0,236,0
thread block = 0,237,0
thread block = 0,238,0
thread block = 0,239,0
thread block = 0,240,0
thread block = 0,241,0
thread block = 0,242,0
thread block = 0,243,0
thread block = 0,244,0
thread block = 0,245,0
thread block = 0,246,0
thread block = 0,247,0
thread block = 0,248,0
thread block = 0,249,0
thread block = 0,250,0
thread block = 0,251,0
thread block = 0,252,0
thread block = 0,253,0
thread block = 0,254,0
thread block = 0,255,0
Destroy streams for kernel 1: size 0
kernel_name = _Z22bpnn_layerforward_CUDAPfS_S_S_ii 
kernel_launch_uid = 1 
kernel_stream_id = 0
gpu_sim_cycle = 8444
gpu_sim_insn = 5894144
gpu_ipc =     698.0275
gpu_tot_sim_cycle = 8444
gpu_tot_sim_insn = 5894144
gpu_tot_ipc =     698.0275
gpu_tot_issued_cta = 256
gpu_occupancy = 90.7037% 
gpu_tot_occupancy = 90.7037% 
max_total_param_size = 0
gpu_stall_dramfull = 0
gpu_stall_icnt2sh    = 0
partiton_level_parallism =       2.6073
partiton_level_parallism_total  =       2.6073
partiton_level_parallism_util =      14.7264
partiton_level_parallism_util_total  =      14.7264
L2_BW  =      94.4467 GB/Sec
L2_BW_total  =      94.4467 GB/Sec
gpu_total_sim_rate=1964714

========= Core cache stats =========
L1I_cache:
	L1I_total_cache_accesses = 0
	L1I_total_cache_misses = 0
	L1I_total_cache_pending_hits = 0
	L1I_total_cache_reservation_fails = 0
L1D_cache:
	L1D_cache_core[0]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 325
	L1D_cache_core[1]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 467
	L1D_cache_core[2]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 442
	L1D_cache_core[3]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 530
	L1D_cache_core[4]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 503
	L1D_cache_core[5]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 525
	L1D_cache_core[6]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 485
	L1D_cache_core[7]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 582
	L1D_cache_core[8]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 520
	L1D_cache_core[9]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 352
	L1D_cache_core[10]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 477
	L1D_cache_core[11]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 406
	L1D_cache_core[12]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 376
	L1D_cache_core[13]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 490
	L1D_cache_core[14]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 451
	L1D_cache_core[15]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 521
	L1D_cache_core[16]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 387
	L1D_cache_core[17]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 520
	L1D_cache_core[18]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 507
	L1D_cache_core[19]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 493
	L1D_cache_core[20]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 367
	L1D_cache_core[21]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 552
	L1D_cache_core[22]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 458
	L1D_cache_core[23]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 546
	L1D_cache_core[24]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 393
	L1D_cache_core[25]: Access = 588, Miss = 240, Miss_rate = 0.408, Pending_hits = 72, Reservation_fails = 574
	L1D_cache_core[26]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 468
	L1D_cache_core[27]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 437
	L1D_cache_core[28]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 354
	L1D_cache_core[29]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 392
	L1D_cache_core[30]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 463
	L1D_cache_core[31]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 403
	L1D_cache_core[32]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 373
	L1D_cache_core[33]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 398
	L1D_cache_core[34]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 384
	L1D_cache_core[35]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 365
	L1D_cache_core[36]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 335
	L1D_cache_core[37]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 436
	L1D_cache_core[38]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 476
	L1D_cache_core[39]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 363
	L1D_cache_core[40]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 451
	L1D_cache_core[41]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 314
	L1D_cache_core[42]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 381
	L1D_cache_core[43]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 359
	L1D_cache_core[44]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 412
	L1D_cache_core[45]: Access = 490, Miss = 200, Miss_rate = 0.408, Pending_hits = 60, Reservation_fails = 377
	L1D_total_cache_accesses = 25088
	L1D_total_cache_misses = 10240
	L1D_total_cache_miss_rate = 0.4082
	L1D_total_cache_pending_hits = 3072
	L1D_total_cache_reservation_fails = 20190
	L1D_cache_data_port_util = 0.080
	L1D_cache_fill_port_util = 0.066
L1C_cache:
	L1C_total_cache_accesses = 0
	L1C_total_cache_misses = 0
	L1C_total_cache_pending_hits = 0
	L1C_total_cache_reservation_fails = 0
L1T_cache:
	L1T_total_cache_accesses = 0
	L1T_total_cache_misses = 0
	L1T_total_cache_pending_hits = 0
	L1T_total_cache_reservation_fails = 0

Total_core_cache_stats:
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][HIT_RESERVED] = 3072
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][MISS] = 2816
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][RESERVATION_FAIL] = 10872
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][SECTOR_MISS] = 6912
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][MSHR_HIT] = 3072
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][HIT] = 11776
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][MISS] = 256
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][RESERVATION_FAIL] = 9318
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][SECTOR_MISS] = 256
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][HIT] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][MISS] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][HIT] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][MISS] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][HIT] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][MISS] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][HIT] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][MISS] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][TOTAL_ACCESS] = 12800
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][TOTAL_ACCESS] = 12288

Total_core_cache_fail_stats:
	Total_core_cache_fail_stats_breakdown[GLOBAL_ACC_R][MISS_QUEUE_FULL] = 10872
	Total_core_cache_fail_stats_breakdown[GLOBAL_ACC_W][MISS_QUEUE_FULL] = 9318
ctas_completed 256, Shader 0 warp_id issue ditsribution:
warp_id:
0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 
distro:
122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 
gpgpu_n_tot_thrd_icount = 7995392
gpgpu_n_tot_w_icount = 249856
gpgpu_n_stall_shd_mem = 4532
gpgpu_n_mem_read_local = 0
gpgpu_n_mem_write_local = 0
gpgpu_n_mem_read_global = 9728
gpgpu_n_mem_write_global = 12288
gpgpu_n_mem_texture = 0
gpgpu_n_mem_const = 0
gpgpu_n_load_insn  = 69632
gpgpu_n_store_insn = 69632
gpgpu_n_shmem_insn = 520192
gpgpu_n_sstarr_insn = 0
gpgpu_n_tex_insn = 0
gpgpu_n_const_mem_insn = 0
gpgpu_n_param_mem_insn = 0
gpgpu_n_shmem_bkconflict = 0
gpgpu_n_l1cache_bkconflict = 447
gpgpu_n_intrawarp_mshr_merge = 0
gpgpu_n_cmem_portconflict = 0
gpgpu_stall_shd_mem[c_mem][resource_stall] = 0
gpgpu_stall_shd_mem[s_mem][bk_conf] = 0
gpgpu_stall_shd_mem[gl_mem][resource_stall] = 447
gpgpu_stall_shd_mem[gl_mem][coal_stall] = 4085
gpgpu_stall_shd_mem[gl_mem][data_port_stall] = 0
gpu_reg_bank_conflict_stalls = 0
Warp Occupancy Distribution:
Stall:241024	W0_Idle:65346	W0_Scoreboard:69718	W1:0	W2:18432	W3:0	W4:0	W5:0	W6:0	W7:0	W8:0	W9:0	W10:0	W11:0	W12:0	W13:0	W14:0	W15:0	W16:18176	W17:0	W18:0	W19:0	W20:0	W21:0	W22:0	W23:0	W24:0	W25:0	W26:0	W27:0	W28:0	W29:0	W30:2048	W31:0	W32:172032
single_issue_nums: WS0:62464	WS1:62464	WS2:62464	WS3:62464	
dual_issue_nums: WS0:0	WS1:0	WS2:0	WS3:0	
traffic_breakdown_coretomem[GLOBAL_ACC_R] = 77824 {8:9728,}
traffic_breakdown_coretomem[GLOBAL_ACC_W] = 491520 {40:12288,}
traffic_breakdown_memtocore[GLOBAL_ACC_R] = 389120 {40:9728,}
traffic_breakdown_memtocore[GLOBAL_ACC_W] = 98304 {8:12288,}
maxmflatency = 1085 
max_icnt2mem_latency = 377 
maxmrqlatency = 200 
max_icnt2sh_latency = 13 
averagemflatency = 574 
avg_icnt2mem_latency = 188 
avg_mrq_latency = 56 
avg_icnt2sh_latency = 3 
mrq_lat_table:1326 	169 	266 	542 	895 	1800 	3698 	522 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
dq_lat_table:0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
mf_lat_table:0 	0 	0 	0 	0 	0 	0 	2802 	10410 	8319 	485 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
icnt2mem_lat_table:0 	0 	0 	0 	0 	4172 	7546 	8583 	1715 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
icnt2sh_lat_table:0 	21055 	912 	49 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
mf_lat_pw_table:0 	0 	0 	0 	0 	0 	0 	1 	2 	2 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
maximum concurrent accesses to same row:
dram[0]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[6]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[7]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[8]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[9]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[10]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[11]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[12]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[13]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[14]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[15]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
maximum service time to same row:
dram[0]:      5607      5607      6161      6171      6229      6239      6292      6295      6436      6418         0         0         0         0         0         0 
dram[1]:      5607      5607      6163      6179      6242      6265      6344      6309      6426      6440         0         0         0         0         0         0 
dram[2]:      5607      5607      6171      6176      6233      6234      6309      6318      6462      6444         0         0         0         0         0         0 
dram[3]:      5607      5607      6159      6158      6227      6227      6296      6293      6399      6430         0         0         0         0         0         0 
dram[4]:      5586      5586      6172      6171      6217      6224      6317      6324      6479      6440         0         0         0         0         0         0 
dram[5]:      5586      5586      6155      6162      6228      6246      6322      6303      6404      6418         0         0         0         0         0         0 
dram[6]:      5586      5586      6170      6159      6229      6261      6298      6320      6424      6430         0         0         0         0         0         0 
dram[7]:      5586      5591      6135      6147      6256      6217      6326      6297      6429      6412         0         0         0         0         0         0 
dram[8]:      5593      5593      6140      6136      6233      6226      6293      6292      6481      6516         0         0         0         0         0         0 
dram[9]:      5593      5593      6128      6127      6230      6226      6328      6306      6493      6445         0         0         0         0         0         0 
dram[10]:      5593      5586      6121      6114      6235      6228      6313      6297      6467      6495         0         0         0         0         0         0 
dram[11]:      5586      5586      6110      6134      6224      6230      6301      6303      6462      6458         0         0         0         0         0         0 
dram[12]:      5600      5600      6161      6157      6215      6220      6320      6319      6486      6476         0         0         0         0         0         0 
dram[13]:      5600      5600      6133      6125      6231      6223      6346      6283      6447      6466         0         0         0         0         0         0 
dram[14]:      5593      5593      6136      6137      6226      6228      6370      6332      6466      6506         0         0         0         0         0         0 
dram[15]:      5593      5593      6118      6113      6226      6225      6317      6319      6462      6450         0         0         0         0         0         0 
average row accesses per activate:
dram[0]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[1]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[2]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[3]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[4]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[5]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[6]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[7]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[8]: 60.000000 60.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[9]: 61.000000 60.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[10]: 60.000000 60.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 35.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[11]: 60.000000 60.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 32.000000 32.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[12]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 36.000000 36.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[13]: 62.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 36.000000 36.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[14]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 36.000000 36.000000      -nan      -nan      -nan      -nan      -nan      -nan 
dram[15]: 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 64.000000 36.000000 36.000000      -nan      -nan      -nan      -nan      -nan      -nan 
average row locality = 9218/160 = 57.612499
number of total memory accesses made:
dram[0]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[6]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[7]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[8]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[9]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[10]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[11]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[12]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[13]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[14]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[15]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
total accesses: 0
min_bank_accesses = 0!
min_chip_accesses = 0!
number of total read accesses:
dram[0]:        64        64        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[1]:        64        64        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[2]:        64        64        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[3]:        64        64        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[4]:        64        64        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[5]:        64        64        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[6]:        64        64        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[7]:        64        64        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[8]:        60        60        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[9]:        61        60        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[10]:        60        60        64        64        64        64        64        64        35        32         0         0         0         0         0         0 
dram[11]:        60        60        64        64        64        64        64        64        32        32         0         0         0         0         0         0 
dram[12]:        64        64        64        64        64        64        64        64        36        36         0         0         0         0         0         0 
dram[13]:        62        64        64        64        64        64        64        64        36        36         0         0         0         0         0         0 
dram[14]:        64        64        64        64        64        64        64        64        36        36         0         0         0         0         0         0 
dram[15]:        64        64        64        64        64        64        64        64        36        36         0         0         0         0         0         0 
total dram reads = 9218
min_bank_accesses = 0!
chip skew: 584/568 = 1.03
number of total write accesses:
dram[0]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[6]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[7]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[8]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[9]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[10]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[11]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[12]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[13]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[14]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[15]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
total dram writes = 0
min_bank_accesses = 0!
min_chip_accesses = 0!
average mf latency per bank:
dram[0]:        828       874       976      1048      1180      1169      1260      1310      2021      2064    none      none      none      none      none      none  
dram[1]:        837       853       991      1038      1194      1188      1285      1335      2102      2100    none      none      none      none      none      none  
dram[2]:        869       858       972      1047      1185      1199      1291      1318      1952      1937    none      none      none      none      none      none  
dram[3]:        875       863       989      1062      1212      1205      1276      1314      1933      1952    none      none      none      none      none      none  
dram[4]:        846       883      1042      1052      1203      1214      1281      1352      1984      2140    none      none      none      none      none      none  
dram[5]:        850       896      1031      1038      1225      1233      1317      1310      1980      2096    none      none      none      none      none      none  
dram[6]:        821       868      1030      1029      1177      1204      1276      1336      1996      2014    none      none      none      none      none      none  
dram[7]:        826       868      1045      1049      1184      1201      1282      1327      1941      1937    none      none      none      none      none      none  
dram[8]:        867       837      1058      1019      1158      1174      1322      1348      2106      2243    none      none      none      none      none      none  
dram[9]:        848       851      1052      1010      1164      1175      1276      1306      2248      2223    none      none      none      none      none      none  
dram[10]:        871       855      1051      1029      1146      1207      1246      1292      2009      2044    none      none      none      none      none      none  
dram[11]:        854       864      1047      1010      1143      1186      1263      1319      1964      2009    none      none      none      none      none      none  
dram[12]:        864       820      1030      1039      1196      1192      1288      1287      1786      1821    none      none      none      none      none      none  
dram[13]:        860       840      1014      1026      1195      1167      1296      1268      1823      1808    none      none      none      none      none      none  
dram[14]:        879       826      1017      1034      1165      1188      1264      1281      2003      1941    none      none      none      none      none      none  
dram[15]:        876       825      1013      1016      1149      1214      1248      1281      1967      1934    none      none      none      none      none      none  
maximum mf latency per bank:
dram[0]:        840       853       862      1011      1011      1015       986      1046      1031      1073         0         0         0         0         0         0
dram[1]:        637       638      1017       857      1037      1035      1008      1061      1048      1071         0         0         0         0         0         0
dram[2]:        639       860       856       892      1000      1030      1006      1031      1048      1058         0         0         0         0         0         0
dram[3]:        633       634       902       879      1029      1006       998      1045      1027      1064         0         0         0         0         0         0
dram[4]:        616       690       817       849       998       999      1013      1075      1025      1085         0         0         0         0         0         0
dram[5]:        668       696       853       819      1039      1047      1064      1044      1078      1072         0         0         0         0         0         0
dram[6]:        610       615       847       830      1013       997      1024      1068      1038      1065         0         0         0         0         0         0
dram[7]:        674       612       826       830       992      1017      1025      1084      1044      1062         0         0         0         0         0         0
dram[8]:        747       784       821       964       988       966      1016      1052      1041      1056         0         0         0         0         0         0
dram[9]:        604       625       819       835      1012       983      1022      1059      1036      1067         0         0         0         0         0         0
dram[10]:        626       643       751       818      1019      1009      1022      1065      1035      1075         0         0         0         0         0         0
dram[11]:        600       596       979       954      1008      1015      1010      1063      1025      1075         0         0         0         0         0         0
dram[12]:        637       617       958       980      1019      1002      1025      1049      1042      1075         0         0         0         0         0         0
dram[13]:        779       791       969       981       964       967      1013      1034      1042      1064         0         0         0         0         0         0
dram[14]:        779       819       981       938       994      1004      1015      1024      1047      1067         0         0         0         0         0         0
dram[15]:        769       781       968       871      1014      1010      1001      1036      1045      1067         0         0         0         0         0         0

========= L2 cache stats =========
L2_cache_bank[0]: Access = 686, Miss = 304, Miss_rate = 0.443, Pending_hits = 14, Reservation_fails = 77
L2_cache_bank[1]: Access = 694, Miss = 304, Miss_rate = 0.438, Pending_hits = 19, Reservation_fails = 0
L2_cache_bank[2]: Access = 683, Miss = 304, Miss_rate = 0.445, Pending_hits = 13, Reservation_fails = 9
L2_cache_bank[3]: Access = 693, Miss = 304, Miss_rate = 0.439, Pending_hits = 18, Reservation_fails = 100
L2_cache_bank[4]: Access = 681, Miss = 304, Miss_rate = 0.446, Pending_hits = 12, Reservation_fails = 79
L2_cache_bank[5]: Access = 695, Miss = 304, Miss_rate = 0.437, Pending_hits = 19, Reservation_fails = 0
L2_cache_bank[6]: Access = 683, Miss = 304, Miss_rate = 0.445, Pending_hits = 13, Reservation_fails = 0
L2_cache_bank[7]: Access = 697, Miss = 304, Miss_rate = 0.436, Pending_hits = 20, Reservation_fails = 94
L2_cache_bank[8]: Access = 693, Miss = 304, Miss_rate = 0.439, Pending_hits = 19, Reservation_fails = 77
L2_cache_bank[9]: Access = 685, Miss = 304, Miss_rate = 0.444, Pending_hits = 16, Reservation_fails = 61
L2_cache_bank[10]: Access = 691, Miss = 304, Miss_rate = 0.440, Pending_hits = 17, Reservation_fails = 75
L2_cache_bank[11]: Access = 683, Miss = 304, Miss_rate = 0.445, Pending_hits = 14, Reservation_fails = 61
L2_cache_bank[12]: Access = 692, Miss = 304, Miss_rate = 0.439, Pending_hits = 16, Reservation_fails = 104
L2_cache_bank[13]: Access = 682, Miss = 304, Miss_rate = 0.446, Pending_hits = 14, Reservation_fails = 0
L2_cache_bank[14]: Access = 694, Miss = 304, Miss_rate = 0.438, Pending_hits = 18, Reservation_fails = 3
L2_cache_bank[15]: Access = 685, Miss = 304, Miss_rate = 0.444, Pending_hits = 16, Reservation_fails = 87
L2_cache_bank[16]: Access = 702, Miss = 304, Miss_rate = 0.433, Pending_hits = 20, Reservation_fails = 6
L2_cache_bank[17]: Access = 695, Miss = 304, Miss_rate = 0.437, Pending_hits = 15, Reservation_fails = 70
L2_cache_bank[18]: Access = 700, Miss = 305, Miss_rate = 0.436, Pending_hits = 18, Reservation_fails = 0
L2_cache_bank[19]: Access = 692, Miss = 304, Miss_rate = 0.439, Pending_hits = 14, Reservation_fails = 96
L2_cache_bank[20]: Access = 681, Miss = 300, Miss_rate = 0.441, Pending_hits = 17, Reservation_fails = 78
L2_cache_bank[21]: Access = 679, Miss = 303, Miss_rate = 0.446, Pending_hits = 12, Reservation_fails = 2
L2_cache_bank[22]: Access = 683, Miss = 300, Miss_rate = 0.439, Pending_hits = 18, Reservation_fails = 34
L2_cache_bank[23]: Access = 673, Miss = 300, Miss_rate = 0.446, Pending_hits = 13, Reservation_fails = 60
L2_cache_bank[24]: Access = 676, Miss = 304, Miss_rate = 0.450, Pending_hits = 14, Reservation_fails = 83
L2_cache_bank[25]: Access = 682, Miss = 304, Miss_rate = 0.446, Pending_hits = 18, Reservation_fails = 20
L2_cache_bank[26]: Access = 669, Miss = 302, Miss_rate = 0.451, Pending_hits = 14, Reservation_fails = 11
L2_cache_bank[27]: Access = 681, Miss = 304, Miss_rate = 0.446, Pending_hits = 18, Reservation_fails = 72
L2_cache_bank[28]: Access = 692, Miss = 308, Miss_rate = 0.445, Pending_hits = 14, Reservation_fails = 0
L2_cache_bank[29]: Access = 702, Miss = 308, Miss_rate = 0.439, Pending_hits = 17, Reservation_fails = 57
L2_cache_bank[30]: Access = 691, Miss = 308, Miss_rate = 0.446, Pending_hits = 14, Reservation_fails = 80
L2_cache_bank[31]: Access = 701, Miss = 308, Miss_rate = 0.439, Pending_hits = 16, Reservation_fails = 7
L2_total_cache_accesses = 22016
L2_total_cache_misses = 9730
L2_total_cache_miss_rate = 0.4420
L2_total_cache_pending_hits = 510
L2_total_cache_reservation_fails = 1503
L2_total_cache_breakdown:
	L2_cache_stats_breakdown[GLOBAL_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][HIT_RESERVED] = 510
	L2_cache_stats_breakdown[GLOBAL_ACC_R][MISS] = 2306
	L2_cache_stats_breakdown[GLOBAL_ACC_R][RESERVATION_FAIL] = 1503
	L2_cache_stats_breakdown[GLOBAL_ACC_R][SECTOR_MISS] = 6912
	L2_cache_stats_breakdown[GLOBAL_ACC_R][MSHR_HIT] = 510
	L2_cache_stats_breakdown[LOCAL_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][HIT] = 11776
	L2_cache_stats_breakdown[GLOBAL_ACC_W][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][MISS] = 128
	L2_cache_stats_breakdown[GLOBAL_ACC_W][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][SECTOR_MISS] = 384
	L2_cache_stats_breakdown[GLOBAL_ACC_W][MSHR_HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][HIT] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][MISS] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][HIT] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][MISS] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][MSHR_HIT] = 0
	L2_cache_stats_breakdown[INST_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[INST_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[INST_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[INST_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[INST_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[INST_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][HIT] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][MISS] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][HIT] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][MISS] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][TOTAL_ACCESS] = 9728
	L2_cache_stats_breakdown[GLOBAL_ACC_W][TOTAL_ACCESS] = 12288
L2_total_cache_reservation_fail_breakdown:
	L2_cache_stats_fail_breakdown[GLOBAL_ACC_R][MSHR_ENRTY_FAIL] = 1503
L2_cache_data_port_util = 0.044
L2_cache_fill_port_util = 0.034

icnt_total_pkts_mem_to_simt=22016
icnt_total_pkts_simt_to_mem=22016
LD_mem_lat_dist  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
ST_mem_lat_dist  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
----------------------------Interconnect-DETAILS--------------------------------
Req_Network_injected_packets_num = 22016
Req_Network_cycles = 8444
Req_Network_injected_packets_per_cycle =       2.6073 
Req_Network_conflicts_per_cycle =       1.7643
Req_Network_conflicts_per_cycle_util =       9.9652
Req_Bank_Level_Parallism =      14.7264
Req_Network_in_buffer_full_per_cycle =       0.0000
Req_Network_in_buffer_avg_util =       3.9182
Req_Network_out_buffer_full_per_cycle =       0.0000
Req_Network_out_buffer_avg_util =       0.0815

Reply_Network_injected_packets_num = 22016
Reply_Network_cycles = 8444
Reply_Network_injected_packets_per_cycle =        2.6073
Reply_Network_conflicts_per_cycle =        0.2652
Reply_Network_conflicts_per_cycle_util =       1.2474
Reply_Bank_Level_Parallism =      12.2652
Reply_Network_in_buffer_full_per_cycle =       0.0000
Reply_Network_in_buffer_avg_util =       0.0163
Reply_Network_out_buffer_full_per_cycle =       0.0000
Reply_Network_out_buffer_avg_util =       0.0567
----------------------------END-of-Interconnect-DETAILS-------------------------


gpgpu_simulation_time = 0 days, 0 hrs, 0 min, 3 sec (3 sec)
gpgpu_simulation_rate = 1964714 (inst/sec)
gpgpu_simulation_rate = 2814 (cycle/sec)
gpgpu_silicon_slowdown = 402274x
launching memcpy command : MemcpyHtoD,0x00007f15b8f4c600,68
launching memcpy command : MemcpyHtoD,0x00007f15b8f4c800,278596
launching memcpy command : MemcpyHtoD,0x00007f15b8e04400,278596
Processing kernel /root/Repos/shared/accel-sim-framework/hw_run/traces/device-0/12.8/backprop-rodinia-2.0-ft/4096___data_result_4096_txt/traces/kernel-2-ctx_0x55b7bc2df3e0.traceg.xz
-kernel name = _Z24bpnn_adjust_weights_cudaPfiS_iS_S_
-kernel id = 2
-grid dim = (1,256,1)
-block dim = (16,16,1)
-shmem = 0
-nregs = 27
-binary version = 86
-cuda stream id = 0
-shmem base_addr = 0x00007f15d5000000
-local mem base_addr = 0x00007f15d3000000
-nvbit version = 1.7.6
-accelsim tracer version = 5
-enable lineinfo = 0
Header info loaded for kernel command : /root/Repos/shared/accel-sim-framework/hw_run/traces/device-0/12.8/backprop-rodinia-2.0-ft/4096___data_result_4096_txt/traces/kernel-2-ctx_0x55b7bc2df3e0.traceg.xz
launching kernel name: _Z24bpnn_adjust_weights_cudaPfiS_iS_S_ uid: 2 cuda_stream_id: 0
GPGPU-Sim uArch: Shader 26 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
GPGPU-Sim: Reconfigure L1 cache to 128KB
thread block = 0,0,0
GPGPU-Sim uArch: Shader 27 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,1,0
GPGPU-Sim uArch: Shader 28 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,2,0
GPGPU-Sim uArch: Shader 29 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,3,0
GPGPU-Sim uArch: Shader 30 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,4,0
GPGPU-Sim uArch: Shader 31 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,5,0
GPGPU-Sim uArch: Shader 32 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,6,0
GPGPU-Sim uArch: Shader 33 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,7,0
GPGPU-Sim uArch: Shader 34 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,8,0
GPGPU-Sim uArch: Shader 35 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,9,0
GPGPU-Sim uArch: Shader 36 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,10,0
GPGPU-Sim uArch: Shader 37 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,11,0
GPGPU-Sim uArch: Shader 38 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,12,0
GPGPU-Sim uArch: Shader 39 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,13,0
GPGPU-Sim uArch: Shader 40 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,14,0
GPGPU-Sim uArch: Shader 41 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,15,0
GPGPU-Sim uArch: Shader 42 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,16,0
GPGPU-Sim uArch: Shader 43 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,17,0
GPGPU-Sim uArch: Shader 44 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,18,0
GPGPU-Sim uArch: Shader 45 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,19,0
GPGPU-Sim uArch: Shader 0 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,20,0
GPGPU-Sim uArch: Shader 1 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,21,0
GPGPU-Sim uArch: Shader 2 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,22,0
GPGPU-Sim uArch: Shader 3 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,23,0
GPGPU-Sim uArch: Shader 4 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,24,0
GPGPU-Sim uArch: Shader 5 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,25,0
GPGPU-Sim uArch: Shader 6 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,26,0
GPGPU-Sim uArch: Shader 7 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,27,0
GPGPU-Sim uArch: Shader 8 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,28,0
GPGPU-Sim uArch: Shader 9 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,29,0
GPGPU-Sim uArch: Shader 10 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,30,0
GPGPU-Sim uArch: Shader 11 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,31,0
GPGPU-Sim uArch: Shader 12 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,32,0
GPGPU-Sim uArch: Shader 13 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,33,0
GPGPU-Sim uArch: Shader 14 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,34,0
GPGPU-Sim uArch: Shader 15 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,35,0
GPGPU-Sim uArch: Shader 16 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,36,0
GPGPU-Sim uArch: Shader 17 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,37,0
GPGPU-Sim uArch: Shader 18 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,38,0
GPGPU-Sim uArch: Shader 19 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,39,0
GPGPU-Sim uArch: Shader 20 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,40,0
GPGPU-Sim uArch: Shader 21 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,41,0
GPGPU-Sim uArch: Shader 22 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,42,0
GPGPU-Sim uArch: Shader 23 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,43,0
GPGPU-Sim uArch: Shader 24 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,44,0
GPGPU-Sim uArch: Shader 25 bind to kernel 2 '_Z24bpnn_adjust_weights_cudaPfiS_iS_S_'
thread block = 0,45,0
thread block = 0,46,0
thread block = 0,47,0
thread block = 0,48,0
thread block = 0,49,0
thread block = 0,50,0
thread block = 0,51,0
thread block = 0,52,0
thread block = 0,53,0
thread block = 0,54,0
thread block = 0,55,0
thread block = 0,56,0
thread block = 0,57,0
thread block = 0,58,0
thread block = 0,59,0
thread block = 0,60,0
thread block = 0,61,0
thread block = 0,62,0
thread block = 0,63,0
thread block = 0,64,0
thread block = 0,65,0
thread block = 0,66,0
thread block = 0,67,0
thread block = 0,68,0
thread block = 0,69,0
thread block = 0,70,0
thread block = 0,71,0
thread block = 0,72,0
thread block = 0,73,0
thread block = 0,74,0
thread block = 0,75,0
thread block = 0,76,0
thread block = 0,77,0
thread block = 0,78,0
thread block = 0,79,0
thread block = 0,80,0
thread block = 0,81,0
thread block = 0,82,0
thread block = 0,83,0
thread block = 0,84,0
thread block = 0,85,0
thread block = 0,86,0
thread block = 0,87,0
thread block = 0,88,0
thread block = 0,89,0
thread block = 0,90,0
thread block = 0,91,0
thread block = 0,92,0
thread block = 0,93,0
thread block = 0,94,0
thread block = 0,95,0
thread block = 0,96,0
thread block = 0,97,0
thread block = 0,98,0
thread block = 0,99,0
thread block = 0,100,0
thread block = 0,101,0
thread block = 0,102,0
thread block = 0,103,0
thread block = 0,104,0
thread block = 0,105,0
thread block = 0,106,0
thread block = 0,107,0
thread block = 0,108,0
thread block = 0,109,0
thread block = 0,110,0
thread block = 0,111,0
thread block = 0,112,0
thread block = 0,113,0
thread block = 0,114,0
thread block = 0,115,0
thread block = 0,116,0
thread block = 0,117,0
thread block = 0,118,0
thread block = 0,119,0
thread block = 0,120,0
thread block = 0,121,0
thread block = 0,122,0
thread block = 0,123,0
thread block = 0,124,0
thread block = 0,125,0
thread block = 0,126,0
thread block = 0,127,0
thread block = 0,128,0
thread block = 0,129,0
thread block = 0,130,0
thread block = 0,131,0
thread block = 0,132,0
thread block = 0,133,0
thread block = 0,134,0
thread block = 0,135,0
thread block = 0,136,0
thread block = 0,137,0
thread block = 0,138,0
thread block = 0,139,0
thread block = 0,140,0
thread block = 0,141,0
thread block = 0,142,0
thread block = 0,143,0
thread block = 0,144,0
thread block = 0,145,0
thread block = 0,146,0
thread block = 0,147,0
thread block = 0,148,0
thread block = 0,149,0
thread block = 0,150,0
thread block = 0,151,0
thread block = 0,152,0
thread block = 0,153,0
thread block = 0,154,0
thread block = 0,155,0
thread block = 0,156,0
thread block = 0,157,0
thread block = 0,158,0
thread block = 0,159,0
thread block = 0,160,0
thread block = 0,161,0
thread block = 0,162,0
thread block = 0,163,0
thread block = 0,164,0
thread block = 0,165,0
thread block = 0,166,0
thread block = 0,167,0
thread block = 0,168,0
thread block = 0,169,0
thread block = 0,170,0
thread block = 0,171,0
thread block = 0,172,0
thread block = 0,173,0
thread block = 0,174,0
thread block = 0,175,0
thread block = 0,176,0
thread block = 0,177,0
thread block = 0,178,0
thread block = 0,179,0
thread block = 0,180,0
thread block = 0,181,0
thread block = 0,182,0
thread block = 0,183,0
thread block = 0,184,0
thread block = 0,185,0
thread block = 0,186,0
thread block = 0,187,0
thread block = 0,188,0
thread block = 0,189,0
thread block = 0,190,0
thread block = 0,191,0
thread block = 0,192,0
thread block = 0,193,0
thread block = 0,194,0
thread block = 0,195,0
thread block = 0,196,0
thread block = 0,197,0
thread block = 0,198,0
thread block = 0,199,0
thread block = 0,200,0
thread block = 0,201,0
thread block = 0,202,0
thread block = 0,203,0
thread block = 0,204,0
thread block = 0,205,0
thread block = 0,206,0
thread block = 0,207,0
thread block = 0,208,0
thread block = 0,209,0
thread block = 0,210,0
thread block = 0,211,0
thread block = 0,212,0
thread block = 0,213,0
thread block = 0,214,0
thread block = 0,215,0
thread block = 0,216,0
thread block = 0,217,0
thread block = 0,218,0
thread block = 0,219,0
thread block = 0,220,0
thread block = 0,221,0
thread block = 0,222,0
thread block = 0,223,0
thread block = 0,224,0
thread block = 0,225,0
thread block = 0,226,0
thread block = 0,227,0
thread block = 0,228,0
thread block = 0,229,0
thread block = 0,230,0
thread block = 0,231,0
thread block = 0,232,0
thread block = 0,233,0
thread block = 0,234,0
thread block = 0,235,0
thread block = 0,236,0
thread block = 0,237,0
thread block = 0,238,0
thread block = 0,239,0
thread block = 0,240,0
thread block = 0,241,0
thread block = 0,242,0
thread block = 0,243,0
thread block = 0,244,0
thread block = 0,245,0
thread block = 0,246,0
thread block = 0,247,0
thread block = 0,248,0
thread block = 0,249,0
thread block = 0,250,0
thread block = 0,251,0
thread block = 0,252,0
thread block = 0,253,0
thread block = 0,254,0
thread block = 0,255,0


GPGPU-Sim uArch: ERROR ** deadlock detected: last writeback core 42 @ gpu_sim_cycle 13541 (+ gpu_tot_sim_cycle 4294875740) (86459 cycles ago)
GPGPU-Sim uArch: DEADLOCK  shader cores no longer committing instructions [core(# threads)]:
GPGPU-Sim uArch: DEADLOCK  7(256) 

Re-run the simulator in gdb and use debug routines in .gdbinit to debug this
